{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecommerce dataset preprocessing\n",
    "\n",
    "Notebook aimed at preprocessing an ecommerce dataset. \n",
    "\n",
    "A new subset is created containing a 100 rows and 4 columns, ['product_name', 'review_title', 'review_text', 'review_rating']. \n",
    "\n",
    "The language of the text is detected using a language detection library. A language detection column is added to the df.\n",
    "\n",
    "An API call is made to translate all reviews into English. Two new columns are added for the translation of the review text and the translation of the review title.\n",
    "\n",
    "The final dataframe has the columns: ['product_name', 'review_title', 'review_text', 'review_rating', 'language', 'translated_text', 'translated_title']. \n",
    "\n",
    "This preprocessed dataframe will be stored a csv locally and will be used from now on in the next working notebooks.\n",
    "\n",
    "###### (Finished on 14.03.2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports required\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import time # Used to pause the API call function to avoid exceeding rate limit\n",
    "from langdetect import detect\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "filename = './data/raw/amazon_uk_dataset.csv'\n",
    "df = pd.read_csv(filename, delimiter=',', index_col=None, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API SDK\n",
    "load_dotenv('../APIopenAI.env')\n",
    "api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe's size reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce number of rows and columns\n",
    "col_to_keep = ['product_name','review_title','review_text','review_rating']\n",
    "df = df.loc[:99, col_to_keep]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [product_name, review_title, review_text, review_rating]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Check dataframe's columns\n",
    "print(df[:0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language review classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df column using a langdetect library to detect language of input text\n",
    "df['language'] = df['review_text'].apply(lambda text : detect(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translation(batch_texts, batch_size):\n",
    "    '''\n",
    "    Params: \n",
    "        batch_texts is an array of texts (str) that need to be translated. Works as prompt.\n",
    "        batch_size the number of texts contained in the array.\n",
    "    Function:\n",
    "        Make an openai API call with instructions to translate to English all text within the array.\n",
    "    Returns: response array in JSON format.\n",
    "    '''\n",
    "\n",
    "    # General system instructions\n",
    "    system_instructions = f\"You will be provided with an array of texts. You have to translate to English the full text.\\\n",
    "            Reply with all full completions in JSON format. The output format should follow the next conditions:  \\\n",
    "            JSON dictionary have as key translations and have as value another dictionary, this second dictionary \\\n",
    "            will have as key the <original text given by user> and as values the <translated text you generated>\\\n",
    "            Output format example: <\\'translations\\': <original text 1: translated text1, original text 2: translated text 2, ...>>\"\n",
    "        \n",
    "    # Call API only for selected texts\n",
    "    response = openai.OpenAI(api_key=api_key).chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_instructions},\n",
    "            {\"role\": \"user\", \"content\": batch_texts}\n",
    "        ],\n",
    "        #max_tokens=128,  # Increase max_tokens to retrieve more than one token\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    # Response is in JSON format\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_translated(new_col: str, batch_set: list, df_non_en):\n",
    "    # Merge the generated translations to a new column on the dataframe \n",
    "    # Create a DataFrame from the set of batches\n",
    "    translated_df = pd.DataFrame({new_col: batch_set}, index=df_non_en.index)\n",
    "    merged_df = pd.merge(df, translated_df, how='left', left_index=True, right_index=True)\n",
    "    # Fill NaN values in the new_col with corresponding values from 'review_title'\n",
    "    merged_df[new_col] = merged_df[new_col].fillna(merged_df['review_title'])\n",
    "    # Add column to original df\n",
    "    df[new_col] = merged_df[new_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_translation(input_col: str):\n",
    "    '''\n",
    "    Main function. \n",
    "    Returns translated texts' list and df with non english rows.\n",
    "    '''\n",
    "    # Separe english and non-english texts\n",
    "    df_non_en = df[df['language'] != 'en']\n",
    "    # For every batch make an API call\n",
    "    batch_size = math.ceil(len(df_non_en)/3) # as 3 is RPM\n",
    "    batch_set = []\n",
    "    for i in range(0, len(df_non_en), batch_size):\n",
    "        # Create batch list containing text chunks\n",
    "        batch_texts = str(list(df_non_en[input_col])[i:i+batch_size])\n",
    "\n",
    "        # Call function with API call, returns an array of translated text\n",
    "        trans_json = batch_translation(batch_texts,batch_size)\n",
    "        trans_json = json.loads(trans_json)\n",
    "\n",
    "        # Transform JSON dict to list of texts\n",
    "        trans_text = list(trans_json['translations'].values())\n",
    "\n",
    "        # Append translated batch to set of translated batches\n",
    "        batch_set.extend(trans_text)\n",
    "    \n",
    "    return batch_set, df_non_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function call for columns to translate\n",
    "batch_set, df_non_en = review_translation('review_text', 'translated_text')\n",
    "# Add translated texts and titles to df columns\n",
    "merge_translated('translated_text', batch_set,df_non_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call for title column\n",
    "batch_set, df_non_en = review_translation('review_title', 'translated_title')\n",
    "merge_translated('translated_title', batch_set,df_non_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 7)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset on current path\n",
    "filename = './data/preprocessed/dataset_pp.csv'\n",
    "df.to_csv(filename, index=False, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
