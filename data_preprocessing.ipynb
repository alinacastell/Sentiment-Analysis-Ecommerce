{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ecommerce dataset preprocessing\n",
    "\n",
    "Notebook aimed at preprocessing an ecommerce dataset. \n",
    "\n",
    "A new subset will be created containing a 100 rows and 4 columns, ['product_name', 'review_title', 'review_text', 'review_rating']. \n",
    "\n",
    "An API call is made to translate all reviews into English. Two new columns will be added for the translation of the review text and the translation of the review title, making the shape of the final df (100,6). \n",
    "\n",
    "This preprocessed dataframe will be stored a csv locally and will be used from now on in the next working notebooks.\n",
    "\n",
    "###### (Finished on 14.03.2024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports required\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "# Used to pause the API call function to avoid exceeding rate limit\n",
    "import time \n",
    "# Language detection library uses the List of ISO 639 language codes\n",
    "from langdetect import detect\n",
    "import json\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset \n",
    "filename = './data/raw/amazon_uk_dataset.csv'\n",
    "og_df = pd.read_csv(filename, delimiter=',', index_col=None, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI API SDK\n",
    "load_dotenv('../APIopenAI.env')\n",
    "api_key = os.getenv('API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataframe's size reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6823, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce number of columns of both dataframes\n",
    "col_to_keep = ['product_name','review_title','review_text','review_rating']\n",
    "df = og_df[col_to_keep].copy()\n",
    "#og_df = og_df[col_to_keep].copy()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Playig around with product name groups\n",
    "df_groups = df.groupby('product_name',as_index=False)['review_title']\n",
    "print(df_groups.ngroups)\n",
    "print(df_groups.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function used to select number of rows grouping by product\n",
    "def row_selection(max_rows):\n",
    "    df_groups = df.groupby('product_name',as_index=False)\n",
    "    for i in range(1,df_groups.ngroups):\n",
    "        rows = int(((df_groups.count()).head(i).sum()[['review_title']]).iloc[0])\n",
    "        if rows > max_rows:\n",
    "            return rows\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aprox_num = 100\n",
    "final_num = row_selection(aprox_num)\n",
    "df = df.loc[:final_num]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language review classification\n",
    "\n",
    "Study both the complete dataset with shape (6823, 4) and the reduced dataset (108, 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply detect function with exception handling\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df column using a langdetect library to detect language of input text\n",
    "df['language'] = df['review_text'].apply(lambda text : detect_language(text) if pd.notnull(text) else None)\n",
    "og_df['language'] = og_df['review_text'].apply(lambda text : detect_language(text) if pd.notnull(text) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct languages in original dataframe: 31\n",
      "Number of distinct languages in reduced dataframe: 8\n"
     ]
    }
   ],
   "source": [
    "# Count number of distinct languages for complete and reduced dataset\n",
    "distinct_lang_og = og_df.groupby('language', as_index=False).count()\n",
    "print(f\"Number of distinct languages in original dataframe: {len(distinct_lang_og['language'])}\")\n",
    "distinct_lang = df.groupby('language', as_index=False).count()\n",
    "print(f\"Number of distinct languages in reduced dataframe: {len(distinct_lang['language'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    de\n",
       "1    en\n",
       "2    es\n",
       "3    fr\n",
       "4    it\n",
       "5    ja\n",
       "6    pt\n",
       "7    tr\n",
       "Name: language, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Previsualisation of language codes of dataframe's first 108 rows\n",
    "distinct_lang['language']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Language Translation using OpenAI gpt-3.5-turbo model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_translation(batch_texts):\n",
    "    '''\n",
    "    Params: \n",
    "        batch_texts is an array of texts (str) that need to be translated. Works as prompt.\n",
    "        batch_size the number of texts contained in the array.\n",
    "    Function:\n",
    "        Make an openai API call with instructions to translate to English all text within the array.\n",
    "    Returns: response array in JSON format.\n",
    "    '''\n",
    "\n",
    "    # General system instructions\n",
    "    system_instructions = f\"You will be provided with an array of texts. You have to translate to \\\n",
    "        English the full text. Reply with all full completions in JSON format. The output format \\\n",
    "        should follow the next conditions:  \\\n",
    "        JSON dictionary have as key translations and have as value another dictionary, this second \\\n",
    "        dictionary will have as key the <original text given by user> and as values the \\\n",
    "        <translated text you generated>. Output format example: <\\'translations\\': \\\n",
    "        <original text 1: translated text1, original text 2: translated text 2, ...>>\"\n",
    "        \n",
    "    # Call API only for selected texts\n",
    "    response = openai.OpenAI(api_key=api_key).chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        response_format={ \"type\": \"json_object\" },\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_instructions},\n",
    "            {\"role\": \"user\", \"content\": batch_texts}\n",
    "        ],\n",
    "        #max_tokens=128,  # Increase max_tokens to retrieve more than one token\n",
    "        n=1,\n",
    "        stop=None\n",
    "    )\n",
    "    # Response is in JSON format\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_translated(new_col, og_col, batch_set, df_non_en):\n",
    "    # Merge the generated translations to a new column on the dataframe \n",
    "    # Create a DataFrame from the set of batches\n",
    "    translated_df = pd.DataFrame({new_col: batch_set}, index=df_non_en.index)\n",
    "    merged_df = pd.merge(df, translated_df, how='left', left_index=True, right_index=True)\n",
    "    # Fill NaN values in the new_col with corresponding values from original column\n",
    "    merged_df[new_col] = merged_df[new_col].fillna(merged_df[og_col])\n",
    "    # Add column to original df\n",
    "    df[new_col] = merged_df[new_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_translation(input_col):\n",
    "    '''\n",
    "    Main function. \n",
    "    Returns translated texts' list and df with non english rows.\n",
    "    '''\n",
    "    # Separe english and non-english texts\n",
    "    df_non_en = df[df['language'] != 'en']\n",
    "    # For every batch make an API call\n",
    "    batch_size = math.ceil(len(df_non_en)/3) # as 3 is RPM\n",
    "    batch_set = []\n",
    "    for i in range(0, len(df_non_en), batch_size):\n",
    "        # Create batch list containing text chunks\n",
    "        batch_texts = str(list(df_non_en[input_col])[i:i+batch_size])\n",
    "\n",
    "        # Call function with API call, returns an array of translated text\n",
    "        trans_json = batch_translation(batch_texts, batch_size)\n",
    "        trans_json = json.loads(trans_json)\n",
    "\n",
    "        # Transform JSON dict to list of texts\n",
    "        trans_text = list(trans_json['translations'].values())\n",
    "\n",
    "        # Append translated batch to set of translated batches\n",
    "        batch_set.extend(trans_text)\n",
    "    \n",
    "    return batch_set, df_non_en\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function call for columns to translate\n",
    "batch_set, df_non_en = review_translation('review_text', 'translated_text')\n",
    "# Add translated texts and titles to df columns\n",
    "merge_translated('translated_text', 'review_text', batch_set, df_non_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call for title column\n",
    "batch_set, df_non_en = review_translation('review_title', 'translated_title')\n",
    "merge_translated('translated_title', 'review_title', batch_set, df_non_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset on current path\n",
    "filename = './data/preprocessed/dataset_pp.csv'\n",
    "df.to_csv(filename, index=False, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
