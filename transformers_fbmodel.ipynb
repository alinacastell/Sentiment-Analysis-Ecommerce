{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Translation using facebook NLLB 200 distilled model \n",
    "\n",
    "Using model loading for easier definition of source and output languages.\n",
    "\n",
    "Using as texts to translate the review titles from preprocessed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and convert to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dictionary mapping languages to language codes\n",
    "language_code_dict = {\n",
    "    'en': 'eng_Latn',\n",
    "    'it': 'ita_Latn',\n",
    "    'es': 'spa_Latn',\n",
    "    'fr': 'fra_Latn',\n",
    "    'de': 'deu_Latn',\n",
    "    'ja': 'jpn_Japn'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts_and_language_codes(df):\n",
    "    # Initialize lists to store texts and language codes\n",
    "    texts = []\n",
    "    language_codes = []\n",
    "    \n",
    "    # Iterate over rows of the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        # Append text to the list\n",
    "        texts.append(row['review_title'])\n",
    "        \n",
    "        # Map language code to language code from dictionary and append to the list\n",
    "        language_codes.append(language_code_dict[row['language']])\n",
    "    \n",
    "    return texts, language_codes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, language_codes = read_texts_and_language_codes(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Definition of general variables for all\n",
    "# Define the model checkpoint\n",
    "model_checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "# Initialize model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, token=True)\n",
    "\n",
    "# Initialize lists to store translated texts\n",
    "translated_texts = []\n",
    "\n",
    "# Iterate through the texts list\n",
    "i = 0 # Temporal iterator for accessing languages_codes list\n",
    "for text in texts:\n",
    "    # Define source language from language codes list\n",
    "    src_lang = str(language_codes[i])\n",
    "\n",
    "    # Initialize tokenizer for input language\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, token=True, src_lang=src_lang)\n",
    "\n",
    "    # Tokenize the text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate translation\n",
    "    translated_tokens = model.generate(\n",
    "        **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"], max_length=30\n",
    "    )\n",
    "    \n",
    "    # Decode translated tokens\n",
    "    translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "    \n",
    "    # Append translated text to the list\n",
    "    translated_texts.append(translated_text)\n",
    "\n",
    "# Print translated texts\n",
    "for original, translated in zip(texts, translated_texts):\n",
    "    print(\"Original Text:\", original)\n",
    "    print(\"Translated Text:\", translated)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
