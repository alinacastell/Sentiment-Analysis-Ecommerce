{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import openai\n",
    "import os\n",
    "import time # Used to pause the API call function to avoid exceeding rate limit\n",
    "import json\n",
    "import math\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM, \\\n",
    "    M2M100ForConditionalGeneration, M2M100Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP BY STEP:\n",
    "- Create pipeline comprising: data preprocess for desired number of rows and columns\n",
    "- Creation of dataframe with reviews translated as rows:\n",
    "    - Gold Standard column: call to OpenAI API gpt-3.5-turbo-0125 notebook\n",
    "    - Helsinki model column: call to Helsinki OPUS-MT notebook\n",
    "    - Facebook model column: call to Facebook NLLB-200-distilled-600M notebook\n",
    "    - Facebook model column: call to Facebook M2M100-418M notebook\n",
    "- Evaluation of models' performance:\n",
    "    - Call to three different notebooks with three metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns of non_en dataset: product_name, review_title, review_text, review_rating, language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preprocessing\n",
    "def preprocess_data(filename, cols):\n",
    "    \"\"\"\n",
    "    Preprocesses the input data.\n",
    "    \n",
    "    Args:\n",
    "    - filename (str): Path to the input CSV file.\n",
    "    - cols (list of str): Selection of columns from the input data.\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame: non English dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(filename, delimiter=',', index_col=None, header=0)\n",
    "    df = df[cols].copy()\n",
    "\n",
    "    # Apply detect function with exception handling\n",
    "    def detect_language(text):\n",
    "        try:\n",
    "            return detect(text)\n",
    "        except Exception:\n",
    "            return None\n",
    "    # Create df column using a langdetect library to detect language of input text\n",
    "    df['language'] = df['review_text'].apply(lambda text : detect_language(text) if pd.notnull(text) else None)\n",
    "\n",
    "    # For better understanding\n",
    "    distinct_lang = df.groupby('language', as_index=False).count()\n",
    "    print(f\"Number of distinct languages in reduced dataframe: {len(distinct_lang['language'])}\")\n",
    "    print(distinct_lang['language'])\n",
    "    \n",
    "    # Separe english and non-english texts\n",
    "    df_non_en = df[df['language'] != 'en']\n",
    "\n",
    "    return df, df_non_en\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Translation using OpenAI API gpt-3.5-turbo-0125 (our Gold Standard model)\n",
    "def translate_openaigpt(reviews, langs):\n",
    "    \"\"\"\n",
    "    Translates reviews using the OpenAI API gpt-3.5-turbo-0125 model.\n",
    "    Makes use of subfunctions.\n",
    "\n",
    "    Args:\n",
    "    - reviews (list): List of reviews to translate.\n",
    "    - langs (list): list of reviews' languages.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Translated reviews.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read API key file\n",
    "    f = open('../APIopenAI.txt','r')\n",
    "    api_key = f.read()\n",
    "\n",
    "    # Function makes OpenAI API call\n",
    "    def batch_translation(batch_texts):\n",
    "        '''\n",
    "        Params: \n",
    "            batch_texts is an array of texts (str) that need to be translated. Works as prompt.\n",
    "            batch_size the number of texts contained in the array.\n",
    "        Function:\n",
    "            Make an openai API call with instructions to translate to English all text within the array.\n",
    "        Returns: response array in JSON format.\n",
    "        '''\n",
    "\n",
    "        # General system instructions\n",
    "        system_instructions = f\"You will be provided with an array of texts. You have to translate to \\\n",
    "            English the full text. Reply with all full completions in JSON format. The output format \\\n",
    "            should follow the next conditions:  \\\n",
    "            JSON dictionary have as key translations and have as value another dictionary, this second \\\n",
    "            dictionary will have as key the <original text given by user> and as values the \\\n",
    "            <translated text you generated>. Output format example: <\\'translations\\': \\\n",
    "            <original text 1: translated text1, original text 2: translated text 2, ...>>\"\n",
    "            \n",
    "        # Call API only for selected texts\n",
    "        response = openai.OpenAI(api_key=api_key).chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-0125\",\n",
    "            response_format={ \"type\": \"json_object\" },\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_instructions},\n",
    "                {\"role\": \"user\", \"content\": batch_texts}\n",
    "            ],\n",
    "            #max_tokens=128,  # Increase max_tokens to retrieve more than one token\n",
    "            n=1,\n",
    "            stop=None\n",
    "        )\n",
    "        print(f\"HEY HEY this is the gpt response {response.choices[0].message.content}\\n\")\n",
    "        # Response is in JSON format\n",
    "        return response.choices[0].message.content, response.usage.prompt_tokens, response.usage.completion_tokens\n",
    "\n",
    "    # Function creates a set of translated batches\n",
    "    def review_translation(input_col):\n",
    "        '''\n",
    "        Main function. \n",
    "        Returns translated texts' list.\n",
    "        '''\n",
    "        tokens = 0\n",
    "        # Call function with API call, returns an array of translated text\n",
    "        trans_json, prompt_tokens, completion_tokens = batch_translation(str(reviews))\n",
    "        trans_json = json.loads(trans_json)\n",
    "        tokens += prompt_tokens\n",
    "        tokens +=completion_tokens\n",
    "        print(f\"tokens used are now {tokens}\\n\")\n",
    "        # Transform JSON dict to list of texts\n",
    "        trans_text = list(trans_json['translations'].values())\n",
    "        return trans_text\n",
    "\n",
    "    # Main function call for columns to translate\n",
    "    batch_set = review_translation(reviews)\n",
    "    return batch_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Translation using Helsinki model\n",
    "def translate_helsinki(non_en_data):\n",
    "    \"\"\"\n",
    "    Translates reviews using the Helsinki/OPUS-MT models.\n",
    "    \n",
    "    Args:\n",
    "    - reviews (list): List of reviews to translate.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Translated reviews.\n",
    "    \"\"\"\n",
    "\n",
    "    # Group by language the non English dataframe\n",
    "    grouped_data = non_en_data.groupby(['language']).apply(lambda x: x.sort_values(['language'], ascending=True))\n",
    "    grouped_data_counts = non_en_data.groupby(['language']).size().reset_index(name='counts')\n",
    "    print(f\"ordered by language {grouped_data.head(5)}\")\n",
    "    \n",
    "    # Boolean function definition to select romance languages\n",
    "    def languages_contain(language):\n",
    "        languages = ['it', 'ca', 'rm', 'es', 'ro', 'gl', 'co', 'wa', 'pt', 'oc', 'an', 'id', 'fr', 'ht', 'roa', 'en']\n",
    "        return language in languages\n",
    "\n",
    "    # Initialize dictionary to store translated texts by language\n",
    "    translated_texts_by_language = {}\n",
    "\n",
    "    # Loop through all texts to be translated\n",
    "    for i in range(len(grouped_data)):\n",
    "        text = grouped_data['review_title'].iloc[i]\n",
    "        language = grouped_data['language'].iloc[i]\n",
    "        \n",
    "        if languages_contain(language):\n",
    "            model_checkpoint = 'Helsinki-NLP/opus-mt-roa-en'\n",
    "        else:\n",
    "            model_checkpoint = f'Helsinki-NLP/opus-mt-{language}-en'\n",
    "        \n",
    "        # Check if the model checkpoint is already loaded for the language\n",
    "        if language not in translated_texts_by_language:\n",
    "            # Load the translation model for this language using pipeline\n",
    "            translator = pipeline(\"translation\", model=model_checkpoint)\n",
    "            translated_texts_by_language[language] = []\n",
    "            print(f\"model is {model_checkpoint}\\n\")\n",
    "\n",
    "        # Translate the text using the loaded model\n",
    "        translation = translator(text)\n",
    "        translated_text = translation[0]['translation_text']\n",
    "        translated_texts_by_language[language].append(translated_text)\n",
    "\n",
    "        # If all texts in this language have been translated, delete the model from memory\n",
    "        if len(translated_texts_by_language[language]) == grouped_data_counts['counts'].iloc[i]:\n",
    "            del translator\n",
    "\n",
    "    return translated_texts_by_language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Translation using Facebook Facebook M2M100-1.2B\n",
    "def translate_facebook_m2m(reviews, langs):\n",
    "    \"\"\"\n",
    "    Translates reviews using the Facebook model M2M100-1.2B.\n",
    "    \n",
    "    Args:\n",
    "    - reviews (list): List of reviews to translate.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Translated reviews.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize lists to store translated texts\n",
    "    translated_texts = []\n",
    "\n",
    "    model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "    tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "    for i in range(len(reviews)):\n",
    "        tokenizer.src_lang = langs[i]\n",
    "        encoded_text = tokenizer(reviews[i], return_tensors=\"pt\")\n",
    "        generated_tokens = model.generate(**encoded_text, forced_bos_token_id=tokenizer.get_lang_id(\"en\"))\n",
    "        translated_text = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "        # Append translated text to the list\n",
    "        translated_texts.append(translated_text)\n",
    "\n",
    "    return translated_texts\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Translation using Facebook NLLB-200-distilled-600M\n",
    "def translate_facebook_nllb(reviews):\n",
    "    \"\"\"\n",
    "    Translates reviews using the Facebook model NLLB-200-distilled-600M.\n",
    "    \n",
    "    Args:\n",
    "    - reviews (list): List of reviews to translate.\n",
    "    \n",
    "    Returns:\n",
    "    - list: Translated reviews.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Read the token from the JSON file\n",
    "    with open(\"..\\.huggingface\\config.json\", \"r\") as file:\n",
    "        token_data = json.load(file)\n",
    "\n",
    "    # Extract the token value\n",
    "    huggingface_token = token_data[\"huggingface_token\"]\n",
    "\n",
    "    # Define the dictionary mapping languages to language codes (provisional)\n",
    "    language_code_dict = {\n",
    "        'en': 'eng_Latn',\n",
    "        'it': 'ita_Latn',\n",
    "        'es': 'spa_Latn',\n",
    "        'fr': 'fra_Latn',\n",
    "        'de': 'deu_Latn',\n",
    "        'ja': 'jpn_Japn',\n",
    "        'tr': 'tur_Latn',\n",
    "        'pt': 'por_Latn'\n",
    "    }\n",
    "\n",
    "    def read_texts_and_language_codes(df):\n",
    "        # Initialize lists to store texts and language codes\n",
    "        texts = []\n",
    "        language_codes = []\n",
    "        \n",
    "        # Iterate over rows of the dataframe\n",
    "        for index, row in df.iterrows():\n",
    "            # Append text to the list\n",
    "            texts.append(row['review_title'])\n",
    "            \n",
    "            # Map language code to language code from dictionary and append to the list\n",
    "            language_codes.append(language_code_dict[row['language']])\n",
    "        \n",
    "        return texts, language_codes\n",
    "    \n",
    "    texts, language_codes = read_texts_and_language_codes(df_non_en)\n",
    " \n",
    "    # Definition of general variables for all\n",
    "    # Define the model checkpoint\n",
    "    model_checkpoint = \"facebook/nllb-200-distilled-600M\"\n",
    "\n",
    "    # Initialize model\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint, token=huggingface_token)\n",
    "\n",
    "    # Initialize lists to store translated texts\n",
    "    translated_texts = []\n",
    "\n",
    "    # Iterate through the texts list\n",
    "    i = 0 # Temporal iterator for accessing languages_codes list\n",
    "    for text in texts:\n",
    "        # Define source language from language codes list\n",
    "        src_lang = str(language_codes[i])\n",
    "\n",
    "        # Initialize tokenizer for input language\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, token=huggingface_token, src_lang=src_lang)\n",
    "\n",
    "        # Tokenize the text\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "        \n",
    "        # Generate translation\n",
    "        translated_tokens = model.generate(\n",
    "            **inputs, forced_bos_token_id=tokenizer.lang_code_to_id[\"eng_Latn\"], max_length=30\n",
    "        )\n",
    "        \n",
    "        # Decode translated tokens\n",
    "        translated_text = tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]\n",
    "        \n",
    "        # Append translated text to the list\n",
    "        translated_texts.append(translated_text)\n",
    "\n",
    "    # Print translated texts\n",
    "    for original, translated in zip(texts, translated_texts):\n",
    "        print(\"Original Text:\", original)\n",
    "        print(\"Translated Text:\", translated)\n",
    "        print()\n",
    "\n",
    "    return translated_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Evaluation of models' performance\n",
    "def evaluate_performance(bleu, bertscore, geval):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of models using three metrics.\n",
    "    \n",
    "    Args:\n",
    "    - metric1_result (float): Result of metric 1.\n",
    "    - metric2_result (float): Result of metric 2.\n",
    "    - metric3_result (float): Result of metric 3.\n",
    "    \n",
    "    Returns:\n",
    "    - dict: Performance evaluation metrics.\n",
    "    \"\"\"\n",
    "    # Your evaluation code here\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check new df 111\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"PERFETTE!!\": \"PERFECT!!\",\n",
      "        \"delusione\": \"Disappointment\",\n",
      "        \"Molto belle\": \"Very beautiful\",\n",
      "        \"Molto carine e comode\": \"Very cute and comfortable\",\n",
      "        \"Bellissime....peccato per il numero\": \"Beautiful....pity about the size\",\n",
      "        \"sehr sch√∂ner Schuh,\": \"Very nice shoe,\",\n",
      "        \"Super leicht und bequem.\": \"Super light and comfortable.\",\n",
      "        \"Schnelle Lieferung, tolle Ware\": \"Fast delivery, great product\",\n",
      "        \"Sch√∂ne Schuhe\": \"Beautiful shoes\",\n",
      "        \"la coincidencia del objeto real y el anunciado\": \"the coincidence of the real and the advertised object\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 404\n",
      "\n",
      "translated batch is ['PERFECT!!', 'Disappointment', 'Very beautiful', 'Very cute and comfortable', 'Beautiful....pity about the size', 'Very nice shoe,', 'Super light and comfortable.', 'Fast delivery, great product', 'Beautiful shoes', 'the coincidence of the real and the advertised object']\n",
      "\n",
      "check new df 134\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Comodida\": \"Comfortable\",\n",
      "        \"Se ven bien\": \"They look good\",\n",
      "        \"Scarpe di tendenza dal design accattivante\": \"Trendy shoes with captivating design\",\n",
      "        \"Top! Erwartungen erf√ºllt!\": \"Top! Expectations fulfilled!\",\n",
      "        \"Sehr moderne Schuhe\": \"Very modern shoes\",\n",
      "        \"Farbecht, wie auf dem Foto\": \"True to color, as in the photo\",\n",
      "        \"Tolle Schuhe\": \"Great shoes\",\n",
      "        \"Perfekt\": \"Perfect\",\n",
      "        \"Schick und bequem\": \"Stylish and comfortable\",\n",
      "        \"√úberrascht.\": \"Surprised.\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 381\n",
      "\n",
      "translated batch is ['Comfortable', 'They look good', 'Trendy shoes with captivating design', 'Top! Expectations fulfilled!', 'Very modern shoes', 'True to color, as in the photo', 'Great shoes', 'Perfect', 'Stylish and comfortable', 'Surprised.']\n",
      "\n",
      "check new df 130\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Gute Schuhe zum guten Preis\": \"Good shoes at a good price\",\n",
      "        \"Laufschuh\": \"Running shoe\",\n",
      "        \"Zuklein\": \"Too small\",\n",
      "        \"Top\": \"Top\",\n",
      "        \"Wrezatro Herren Laufschuhe Atmungsaktiv rutsch...\": \"Wrezatro Men's Running Shoes Breathable Slip...\",\n",
      "        \"No es de piel. Es sint√©tico.\": \"It is not leather. It is synthetic.\",\n",
      "        \"„Çµ„Ç§„Ç∫„ÅåÔºå„Çè„Åã„Çâ„Å™„ÅÑ„ÄÇ\": \"I don't know the size.\",\n",
      "        \"p√©sima descripci√≥n\": \"terrible description\",\n",
      "        \"Quality\": \"Quality\",\n",
      "        \"Comfortable\": \"Comfortable\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 383\n",
      "\n",
      "translated batch is ['Good shoes at a good price', 'Running shoe', 'Too small', 'Top', \"Wrezatro Men's Running Shoes Breathable Slip...\", 'It is not leather. It is synthetic.', \"I don't know the size.\", 'terrible description', 'Quality', 'Comfortable']\n",
      "\n",
      "check new df 126\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Comfortable for jogging\": \"Comfortable for jogging\",\n",
      "        \"Excellent.\": \"Excellent.\",\n",
      "        \"Se va el color al par de semanas\": \"The color fades after a couple of weeks\",\n",
      "        \"Sehr sch√∂ner leichter Schuh!\": \"Very nice lightweight shoe!\",\n",
      "        \"Sch√∂ne und robust\": \"Beautiful and sturdy\",\n",
      "        \"Perfecto\": \"Perfect\",\n",
      "        \"Taglia sbagliata. Ricevuto 41 e non 42.5\": \"Wrong size. Received 41 instead of 42.5\",\n",
      "        \"Prodotto buono\": \"Good product\",\n",
      "        \"Excelente\": \"Excellent\",\n",
      "        \"Segunda vez que se confunden con la talla\": \"Second time they get the size wrong\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 405\n",
      "\n",
      "translated batch is ['Comfortable for jogging', 'Excellent.', 'The color fades after a couple of weeks', 'Very nice lightweight shoe!', 'Beautiful and sturdy', 'Perfect', 'Wrong size. Received 41 instead of 42.5', 'Good product', 'Excellent', 'Second time they get the size wrong']\n",
      "\n",
      "check new df 91\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Rennsohle\": \"Running sole\",\n",
      "        \"Perfectas muy bonitas\": \"Perfect and very beautiful\",\n",
      "        \"Gigantes y sin caja original\": \"Large and without original box\",\n",
      "        \"Belle e comode\": \"Beautiful and comfortable\",\n",
      "        \"Converse\": \"Converse\",\n",
      "        \"Super\": \"Super\",\n",
      "        \"Super Schuh\": \"Super shoe\",\n",
      "        \"PRECIO EXCELENTE\": \"EXCELLENT PRICE\",\n",
      "        \"Toller und bequemer Sneaker\": \"Great and comfortable sneaker\",\n",
      "        \"Five Stars\": \"Five Stars\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 359\n",
      "\n",
      "translated batch is ['Running sole', 'Perfect and very beautiful', 'Large and without original box', 'Beautiful and comfortable', 'Converse', 'Super', 'Super shoe', 'EXCELLENT PRICE', 'Great and comfortable sneaker', 'Five Stars']\n",
      "\n",
      "check new df 101\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Excelente amo puma\": \"Excellent, I love Puma\",\n",
      "        \"Publicidad enga√±osa pero est√°n bonitos.\": \"Misleading advertising but they look nice.\",\n",
      "        \"Es lo que esperaba\": \"It's what I expected\",\n",
      "        \"Padris√≠mos\": \"Awesome\",\n",
      "        \"Perfecto y casual\": \"Perfect and casual\",\n",
      "        \"Muy buena compra\": \"Very good purchase\",\n",
      "        \"Ottime scarpe per uso quotidiano\": \"Excellent shoes for everyday use\",\n",
      "        \"Comode\": \"Comfortable\",\n",
      "        \"Veste grande\": \"Size runs large\",\n",
      "        \"Passt perfekt.\": \"Fits perfectly.\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 375\n",
      "\n",
      "translated batch is ['Excellent, I love Puma', 'Misleading advertising but they look nice.', \"It's what I expected\", 'Awesome', 'Perfect and casual', 'Very good purchase', 'Excellent shoes for everyday use', 'Comfortable', 'Size runs large', 'Fits perfectly.']\n",
      "\n",
      "check new df 122\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Perfecto üëå\": \"Perfect üëå\",\n",
      "        \"S√∫per c√≥modos y ligeros.\": \"Super comfortable and light.\",\n",
      "        \"En caja y en Perfecto estado\": \"In box and in perfect condition\",\n",
      "        \"purtroppo questo modello forse non √® il top\": \"Unfortunately, this model may not be the top\",\n",
      "        \"Running grand confort\": \"Running great comfort\",\n",
      "        \"Sehr sch√∂ne Schuhe\": \"Very nice shoes\",\n",
      "        \"Bequem und leichter Sneaker\": \"Comfortable and light sneaker\",\n",
      "        \"Eine zwischen Gr√∂√üe w√§re besser gewesen\": \"An in-between size would have been better\",\n",
      "        \"Gut\": \"Good\",\n",
      "        \"Scarpa diadora camaro used\": \"Diadora Camaro used shoe\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 421\n",
      "\n",
      "translated batch is ['Perfect üëå', 'Super comfortable and light.', 'In box and in perfect condition', 'Unfortunately, this model may not be the top', 'Running great comfort', 'Very nice shoes', 'Comfortable and light sneaker', 'An in-between size would have been better', 'Good', 'Diadora Camaro used shoe']\n",
      "\n",
      "check new df 132\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Prodotto molto bello (taglia con vestibilit√† r...\": \"Very nice product (size with good fit...\",\n",
      "        \"Diadora, bitte weiter so\": \"Diadora, please keep it up\",\n",
      "        \"Purtroppo no!\": \"Unfortunately no!\",\n",
      "        \"Giudizio positivo\": \"Positive feedback\",\n",
      "        \"Buone\": \"Good\",\n",
      "        \"Top\": \"Top\",\n",
      "        \"ÂπÖÂ∫É„Åï„ÇìÊ≥£„Åã„Åõ\": \"Wide width makes people cry\",\n",
      "        \"Êï¢„Åà„Å¶Ë®Ä„ÅÜ„Å®„ÄÅÂ∞ë„ÅóÈáç„ÅÑÊÑü„Åò„Åß„Åô„Åã„Å≠„ÄÇ\": \"I dare say it feels a bit heavy.\",\n",
      "        \"„Çµ„Ç§„Ç∫„Å´„Å§„ÅÑ„Å¶\": \"About the size\",\n",
      "        \"ÔΩ∂Ôæú(„Éª‚àÄ„Éª)ÔΩ≤ÔΩ≤!!\": \"Great („Éª‚àÄ„Éª)!\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 451\n",
      "\n",
      "translated batch is ['Very nice product (size with good fit...', 'Diadora, please keep it up', 'Unfortunately no!', 'Positive feedback', 'Good', 'Top', 'Wide width makes people cry', 'I dare say it feels a bit heavy.', 'About the size', 'Great („Éª‚àÄ„Éª)!']\n",
      "\n",
      "check new df 117\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"2034     Sch√∂ne, weiche Sohle\": \"Beautiful, soft sole\",\n",
      "        \"2035           Tienen brillos\": \"They have shine\",\n",
      "        \"2036                  Amplios\": \"Spacious\",\n",
      "        \"2053          Ottimo acquisto\": \"Excellent purchase\",\n",
      "        \"2054          Ottimo prodotto\": \"Excellent product\",\n",
      "        \"2225         Comode e leggere\": \"Comfortable and light\",\n",
      "        \"2226    l√©g√®re et confortable\": \"Light and comfortable\",\n",
      "        \"2245          Leuke schoenen.\": \"Nice shoes.\",\n",
      "        \"2254                Great fit\": \"Great fit\",\n",
      "        \"2269       Excelente producto\": \"Excellent product\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 380\n",
      "\n",
      "translated batch is ['Beautiful, soft sole', 'They have shine', 'Spacious', 'Excellent purchase', 'Excellent product', 'Comfortable and light', 'Light and comfortable', 'Nice shoes.', 'Great fit', 'Excellent product']\n",
      "\n",
      "check new df 102\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Guter Schuh\": \"Good shoe\",\n",
      "        \"Contenta\": \"Happy\",\n",
      "        \"Sneaker\": \"Sneaker\",\n",
      "        \"Buena calidad\": \"Good quality\",\n",
      "        \"parfaites\": \"perfect\",\n",
      "        \"Chaussures au top et styl√©\": \"Top and stylish shoes\",\n",
      "        \"Ergonomischer Schuh\": \"Ergonomic shoe\",\n",
      "        \"Tout bon !\": \"All good!\",\n",
      "        \"Mitig√©e\": \"Mixed feelings\",\n",
      "        \"Geox una garanzia\": \"Geox a guarantee\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 343\n",
      "\n",
      "translated batch is ['Good shoe', 'Happy', 'Sneaker', 'Good quality', 'perfect', 'Top and stylish shoes', 'Ergonomic shoe', 'All good!', 'Mixed feelings', 'Geox a guarantee']\n",
      "\n",
      "check new df 107\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Zapatos comodos, buena opcion para pies sensibles\": \"Comfortable shoes, good option for sensitive feet\",\n",
      "        \"Muy c√≥modos\": \"Very comfortable\",\n",
      "        \"Ich liebe diese Schuhe!\": \"I love these shoes!\",\n",
      "        \"Bester Schuh ever\": \"Best shoe ever\",\n",
      "        \"Sch√∂ner nachhaltiger Schuh\": \"Beautiful sustainable shoe\",\n",
      "        \"Comod√≠simos,frescos y un sujeci√≥n al suelo per\": \"Very comfortable, fresh and good grip on the ground\",\n",
      "        \"Gut\": \"Good\",\n",
      "        \"Sehr sch√∂n angenehme Schuhe\": \"Very nice comfortable shoes\",\n",
      "        \"Sehr bequeme Schuhe\": \"Very comfortable shoes\",\n",
      "        \"Tr√®s jolies\": \"Very pretty\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 429\n",
      "\n",
      "translated batch is ['Comfortable shoes, good option for sensitive feet', 'Very comfortable', 'I love these shoes!', 'Best shoe ever', 'Beautiful sustainable shoe', 'Very comfortable, fresh and good grip on the ground', 'Good', 'Very nice comfortable shoes', 'Very comfortable shoes', 'Very pretty']\n",
      "\n",
      "check new df 120\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Exelentes zapatos\": \"Excellent shoes\",\n",
      "        \"Gute Qualit√§t , ging dennoch zur√ºck\": \"Good quality, but still returned\",\n",
      "        \"ohne Titel\": \"No title\",\n",
      "        \"Ê∞ó„Å´ÂÖ•„Çä„Åæ„Åó„Åü„Åå„Éª„Éª„Éª\": \"I liked it, but...\",\n",
      "        \"Sch√∂ner Gummistiefel der uns passt\": \"Nice rubber boots that fit us\",\n",
      "        \"Bonitos y econ√≥micos\": \"Nice and economical\",\n",
      "        \"Muy bien\": \"Very good\",\n",
      "        \"Top\": \"Top\",\n",
      "        \"PAS IMPERMEABLE LES FLEX XP\": \"NOT WATERPROOF THE FLEX XP\",\n",
      "        \"Ok\": \"Ok\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 387\n",
      "\n",
      "translated batch is ['Excellent shoes', 'Good quality, but still returned', 'No title', 'I liked it, but...', 'Nice rubber boots that fit us', 'Nice and economical', 'Very good', 'Top', 'NOT WATERPROOF THE FLEX XP', 'Ok']\n",
      "\n",
      "check new df 93\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"J‚Äôadore les Reebok, f√©minines et confortables ...\": \"I adore Reebok, feminine and comfortable ...\",\n",
      "        \"Producto confuso en tallas\": \"Confusing product in sizes\",\n",
      "        \"Padr√≠simos!\": \"Awesome!\",\n",
      "        \"excelente servicio\": \"excellent service\",\n",
      "        \"Comodisimas\": \"Very comfortable\",\n",
      "        \"Sehr schmal geschnitten\": \"Very narrow cut\",\n",
      "        \"Buen producto\": \"Good product\",\n",
      "        \"Talla inexacta pero muy comodos\": \"Inaccurate size but very comfortable\",\n",
      "        \"Comode e ben fatte\": \"Comfortable and well made\",\n",
      "        \"Comodo e bella forma\": \"Comfortable and beautiful shape\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 408\n",
      "\n",
      "translated batch is ['I adore Reebok, feminine and comfortable ...', 'Confusing product in sizes', 'Awesome!', 'excellent service', 'Very comfortable', 'Very narrow cut', 'Good product', 'Inaccurate size but very comfortable', 'Comfortable and well made', 'Comfortable and beautiful shape']\n",
      "\n",
      "check new df 79\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Joya de tenis.\": \"Tennis jewel.\",\n",
      "        \"Las tallas vienen demasiado grandes en el caso...\": \"The sizes come too big in the case...\",\n",
      "        \"S√∫per c√≥modos para el ejercicio\": \"Super comfortable for exercise\",\n",
      "        \"Comodos\": \"Comfortable\",\n",
      "        \"Excelentes tenis para correr.\": \"Excellent running shoes.\",\n",
      "        \"Bonne qualit√©\": \"Good quality\",\n",
      "        \"super\": \"super\",\n",
      "        \"Tiras\": \"Straps\",\n",
      "        \"Pour les ballerines...\": \"For the ballerinas...\",\n",
      "        \"Excelentes, pero pedir un n√∫mero m√°s grande! Y...\": \"Excellent, but order a larger size! And...\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 397\n",
      "\n",
      "translated batch is ['Tennis jewel.', 'The sizes come too big in the case...', 'Super comfortable for exercise', 'Comfortable', 'Excellent running shoes.', 'Good quality', 'super', 'Straps', 'For the ballerinas...', 'Excellent, but order a larger size! And...']\n",
      "\n",
      "check new df 110\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Regalo apressato.\": \"Hasty gift.\",\n",
      "        \"Sehr schlechte Qualit√§t\": \"Very poor quality\",\n",
      "        \"Muy bonitas.\": \"Very nice.\",\n",
      "        \"Muy buenos\": \"Very good\",\n",
      "        \"Perfette\": \"Perfect\",\n",
      "        \"Tal y como se describe en el anuncio\": \"Just as described in the advertisement\",\n",
      "        \"todo bien\": \"all good\",\n",
      "        \"No est√° mal\": \"It's not bad\",\n",
      "        \"Pedir medio numero mas del habitual!\": \"Ask for half a size larger than usual!\",\n",
      "        \"C√≥modas, talla correcta, buena calidad\": \"Comfortable, correct size, good quality\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 389\n",
      "\n",
      "translated batch is ['Hasty gift.', 'Very poor quality', 'Very nice.', 'Very good', 'Perfect', 'Just as described in the advertisement', 'all good', \"It's not bad\", 'Ask for half a size larger than usual!', 'Comfortable, correct size, good quality']\n",
      "\n",
      "check new df 130\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Todo perfecto\": \"Everything perfect\",\n",
      "        \"Buen calzado\": \"Good footwear\",\n",
      "        \"Mal tallaje pero preciosas\": \"Wrong size but beautiful\",\n",
      "        \"comodissime\": \"very comfortable\",\n",
      "        \"Excelente compra\": \"Excellent purchase\",\n",
      "        \"Talla justa\": \"Right size\",\n",
      "        \"Justo.\": \"Fair.\",\n",
      "        \"TOP Preis- Leistung\": \"TOP price-performance\",\n",
      "        \"Very good\": \"Very good\",\n",
      "        \"Buena calidad y super precio!!!\": \"Good quality and great price!!!\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 346\n",
      "\n",
      "translated batch is ['Everything perfect', 'Good footwear', 'Wrong size but beautiful', 'very comfortable', 'Excellent purchase', 'Right size', 'Fair.', 'TOP price-performance', 'Very good', 'Good quality and great price!!!']\n",
      "\n",
      "check new df 95\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Perfecta\": \"Perfect\",\n",
      "        \"Buena calidad.\": \"Good quality.\",\n",
      "        \"F√§llt zu schmal aus\": \"Runs too narrow\",\n",
      "        \"Hochwertiger warmer Wintersneaker\": \"High-quality warm winter sneaker\",\n",
      "        \"Sehr sch√∂ner Schuh - Rei√üverschluss verbesseru...\": \"Very nice shoe - zipper improvement...\",\n",
      "        \"Incre√≠ble dise√±o\": \"Incredible design\",\n",
      "        \"Calidad\": \"Quality\",\n",
      "        \"Calidad 100% reebok\": \"100% Reebok quality\",\n",
      "        \"Calidad en los materiales\": \"Quality in the materials\",\n",
      "        \"Muy buen material\": \"Very good material\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 391\n",
      "\n",
      "translated batch is ['Perfect', 'Good quality.', 'Runs too narrow', 'High-quality warm winter sneaker', 'Very nice shoe - zipper improvement...', 'Incredible design', 'Quality', '100% Reebok quality', 'Quality in the materials', 'Very good material']\n",
      "\n",
      "check new df 92\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"angenehm weiches Leder\": \"pleasantly soft leather\",\n",
      "        \"No apto para pies muy anchos\": \"Not suitable for very wide feet\",\n",
      "        \"Schmale Schuhe keine Weite\": \"Narrow shoes, no width\",\n",
      "        \"immer ARA\": \"always ARA\",\n",
      "        \"Prima\": \"Excellent\",\n",
      "        \"Top\": \"Top\",\n",
      "        \"Shoes\": \"Shoes\",\n",
      "        \"Perfecto. Muy bonito\": \"Perfect. Very beautiful\",\n",
      "        \"Muy bonitas y elegantes.\": \"Very beautiful and elegant.\",\n",
      "        \"Chaussures √©l√©gantes et tr√®s confortables.\": \"Elegant and very comfortable shoes.\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 387\n",
      "\n",
      "translated batch is ['pleasantly soft leather', 'Not suitable for very wide feet', 'Narrow shoes, no width', 'always ARA', 'Excellent', 'Top', 'Shoes', 'Perfect. Very beautiful', 'Very beautiful and elegant.', 'Elegant and very comfortable shoes.']\n",
      "\n",
      "check new df 119\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Perfectos. Nota son angostos es para personas ...\": \"Perfect. Note they are narrow for people ...\",\n",
      "        \"Converse gialle alte\": \"Yellow high Converse\",\n",
      "        \"no coincide con el color seleccionado\": \"does not match the selected color\",\n",
      "        \"Kam leider in einer falschen Farbe\": \"Unfortunately came in the wrong color\",\n",
      "        \"Artikel kann ich nur weiter empfehlen\": \"I can only highly recommend the item\",\n",
      "        \"Muy c√≥moda\": \"Very comfortable\",\n",
      "        \"Calidad a buen precio\": \"Quality at a good price\",\n",
      "        \"De bonnes chaussures de running, mais pour la ...\": \"Good running shoes, but for ...\",\n",
      "        \"Insatisfecha\": \"Dissatisfied\",\n",
      "        \"Turnschuhe\": \"Sneakers\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 433\n",
      "\n",
      "translated batch is ['Perfect. Note they are narrow for people ...', 'Yellow high Converse', 'does not match the selected color', 'Unfortunately came in the wrong color', 'I can only highly recommend the item', 'Very comfortable', 'Quality at a good price', 'Good running shoes, but for ...', 'Dissatisfied', 'Sneakers']\n",
      "\n",
      "check new df 96\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Perfecto\": \"Perfect\",\n",
      "        \"Top\": \"Top\",\n",
      "        \"Su utilidadad\": \"Its usefulness\",\n",
      "        \"Zapatillas perfectas\": \"Perfect sneakers\",\n",
      "        \"Sandali di ottima fattura\": \"Sandals of excellent workmanship\",\n",
      "        \"Muy chulas\": \"Very cool\",\n",
      "        \"Todo bien\": \"All good\",\n",
      "        \"Buenas\": \"Good\",\n",
      "        \"Calidad\": \"Quality\",\n",
      "        \"Toller Schuh und Top Verk√§ufer!\": \"Great shoe and top seller!\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 344\n",
      "\n",
      "translated batch is ['Perfect', 'Top', 'Its usefulness', 'Perfect sneakers', 'Sandals of excellent workmanship', 'Very cool', 'All good', 'Good', 'Quality', 'Great shoe and top seller!']\n",
      "\n",
      "check new df 99\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Il prodotto √® ottimo, consegna in un lampo.\": \"The product is excellent, lightning-fast delivery.\",\n",
      "        \"Tolles Design, Gesichtsmaske im Paket\": \"Great design, face mask in the package\",\n",
      "        \"Tutto ok belle!\": \"Everything OK beautiful!\",\n",
      "        \"Sehr gut\": \"Very good\",\n",
      "        \"Bello e confortevole\": \"Beautiful and comfortable\",\n",
      "        \"Tr√®s mauvais produits chaussure ont chang√© la ...\": \"Very bad shoe products have changed ...\",\n",
      "        \"No es de piel. Es sint√©tico.\": \"It is not leather. It is synthetic.\",\n",
      "        \"„Çµ„Ç§„Ç∫„ÅåÔºå„Çè„Åã„Çâ„Å™„ÅÑ„ÄÇ\": \"I do not know the size.\",\n",
      "        \"J‚Äôadore les Reebok, f√©minines et confortables ...\": \"I love Reebok, feminine and comfortable ...\",\n",
      "        \"Excelentes\": \"Excellent\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 471\n",
      "\n",
      "translated batch is ['The product is excellent, lightning-fast delivery.', 'Great design, face mask in the package', 'Everything OK beautiful!', 'Very good', 'Beautiful and comfortable', 'Very bad shoe products have changed ...', 'It is not leather. It is synthetic.', 'I do not know the size.', 'I love Reebok, feminine and comfortable ...', 'Excellent']\n",
      "\n",
      "check new df 99\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Misura completamente sbagliata\": \"Completely wrong size\",\n",
      "        \"16 tallas de diferencia\": \"16 sizes difference\",\n",
      "        \"Talla err√≥nea.\": \"Wrong size.\",\n",
      "        \"Poca durabilidad\": \"Low durability\",\n",
      "        \"Bonitas pero muy duras!\": \"Nice but very hard!\",\n",
      "        \"Est√°n muy bonitos\": \"They look very nice\",\n",
      "        \"Falla\": \"Failure\",\n",
      "        \"Les Rolls des Tongues, les UGG!\": \"The Rolls of Tongues, the UGG!\",\n",
      "        \"einmal getragen und Farbe ab schlechtes quali√§t\": \"worn once and color faded, poor quality\",\n",
      "        \"Gute Verarbeitung. Sieht Wertigkeit aus.\": \"Good workmanship. Looks valuable.\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 419\n",
      "\n",
      "translated batch is ['Completely wrong size', '16 sizes difference', 'Wrong size.', 'Low durability', 'Nice but very hard!', 'They look very nice', 'Failure', 'The Rolls of Tongues, the UGG!', 'worn once and color faded, poor quality', 'Good workmanship. Looks valuable.']\n",
      "\n",
      "check new df 92\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"c√≥modas\": \"comfortable\",\n",
      "        \"Good quality great fit\": \"Good quality great fit\",\n",
      "        \"tr√®s confirtable\": \"very comfortable\",\n",
      "        \"Compra perfecta\": \"perfect purchase\",\n",
      "        \"Bonne qualit√© comme toujours de Teva\": \"Good quality as always from Teva\",\n",
      "        \"Super Produkt\": \"Great product\",\n",
      "        \"Alles ok\": \"Everything ok\",\n",
      "        \"R√°pido y calentitas\": \"Fast and warm\",\n",
      "        \"No me han gustado, las devolv√≠\": \"I didn't like them, I returned them\",\n",
      "        \"la rapidez\": \"the speed\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 373\n",
      "\n",
      "translated batch is ['comfortable', 'Good quality great fit', 'very comfortable', 'perfect purchase', 'Good quality as always from Teva', 'Great product', 'Everything ok', 'Fast and warm', \"I didn't like them, I returned them\", 'the speed']\n",
      "\n",
      "check new df 114\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Je recommande\": \"I recommend\",\n",
      "        \"Entrega antes del tiempo estimado.\": \"Delivery before estimated time.\",\n",
      "        \"Sohle leider viel zu hart\": \"Unfortunately, sole is too hard.\",\n",
      "        \"Sch√∂ner Schuh\": \"Nice shoe\",\n",
      "        \"Gro√üe 26 mit zip an der Seite\": \"Size 26 with zip on the side\",\n",
      "        \"Perfectas\": \"Perfect\",\n",
      "        \"Attache d√©chir√©e apr√®s les avoir port√©es quelq...\": \"Strap torn after wearing them a few times...\",\n",
      "        \"Bonitas pero Inc√≥modas\": \"Pretty but uncomfortable\",\n",
      "        \"La calidad de la marca\": \"The quality of the brand\",\n",
      "        \"Calidad pablosky siempre de 10\": \"Pablosky quality always top-notch\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 425\n",
      "\n",
      "translated batch is ['I recommend', 'Delivery before estimated time.', 'Unfortunately, sole is too hard.', 'Nice shoe', 'Size 26 with zip on the side', 'Perfect', 'Strap torn after wearing them a few times...', 'Pretty but uncomfortable', 'The quality of the brand', 'Pablosky quality always top-notch']\n",
      "\n",
      "check new df 146\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"restitutito per i materiali usati\": \"Refunded for the materials used\",\n",
      "        \"comode e belle\": \"Comfortable and beautiful\",\n",
      "        \"Buon prodotto\": \"Good product\",\n",
      "        \"Buena calidad y dise√±o por el precio\": \"Good quality and design for the price\",\n",
      "        \"Buenos materiales, gran opci√≥n.\": \"Good materials, great option.\",\n",
      "        \"Nunca lleg√≥.\": \"It never arrived.\",\n",
      "        \"Excelente calidad\": \"Excellent quality\",\n",
      "        \"Me gusto mucho\": \"I liked it a lot\",\n",
      "        \"Big and wide for size\": \"Big and spacious for its size\",\n",
      "        \"Superbe!\": \"Superb!\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 385\n",
      "\n",
      "translated batch is ['Refunded for the materials used', 'Comfortable and beautiful', 'Good product', 'Good quality and design for the price', 'Good materials, great option.', 'It never arrived.', 'Excellent quality', 'I liked it a lot', 'Big and spacious for its size', 'Superb!']\n",
      "\n",
      "check new df 137\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"6319    This shoe is fabulous But be sure to go one co...\": \"This shoe is fabulous. But make sure to go one size...\",\n",
      "        \"6323                          Muy C√≥modos, Me Encantaron!\": \"Very comfortable, I loved them!\",\n",
      "        \"6324                                       No me quedaron\": \"They didn't fit me\",\n",
      "        \"6325               Originales y de piel. Buenos acabados.\": \"Original and leather. Good finishes.\",\n",
      "        \"6327                                                 Bien\": \"Good\",\n",
      "        \"6557                  retro scarpa scomodo mal progettato\": \"retro shoe uncomfortable poorly designed\",\n",
      "        \"6558                     Scarpa completa per fuori-strada\": \"Complete shoe for off-road\",\n",
      "        \"6559                                    No los recomiendo\": \"I do not recommend them\",\n",
      "        \"6560                       Delivery time expended 45 days\": \"Delivery time extended to 45 days\",\n",
      "        \"6562                                Una atencion perfecta\": \"A perfect attention\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 468\n",
      "\n",
      "translated batch is ['This shoe is fabulous. But make sure to go one size...', 'Very comfortable, I loved them!', \"They didn't fit me\", 'Original and leather. Good finishes.', 'Good', 'retro shoe uncomfortable poorly designed', 'Complete shoe for off-road', 'I do not recommend them', 'Delivery time extended to 45 days', 'A perfect attention']\n",
      "\n",
      "check new df 100\n",
      "\n",
      "HEY HEY this is the gpt response {\n",
      "    \"translations\": {\n",
      "        \"Strafighe\": \"Beautiful women\",\n",
      "        \"Sch√∂ner Schuh mit nicht ganz so guter Qualit√§t\": \"Nice shoe with not very good quality\",\n",
      "        \"Sehr sch√∂n!\": \"Very beautiful!\",\n",
      "        \"Producto confuso en tallas\": \"Confusing product in sizes\",\n",
      "        \"Padr√≠simos!\": \"Awesome!\",\n",
      "        \"Schick und leicht\": \"Stylish and lightweight\",\n",
      "        \"EXCELLENT\": \"EXCELLENT\",\n",
      "        \"Einfach sch√∂ne Hausschuhe\": \"Simply beautiful slippers\",\n",
      "        \"Langlebig.\": \"Durable.\",\n",
      "        \"Hausschuhe f√ºr lange kalte Winterzeiten.\": \"Slippers for long cold winter times.\"\n",
      "    }\n",
      "}\n",
      "\n",
      "tokens used are now 400\n",
      "\n",
      "translated batch is ['Beautiful women', 'Nice shoe with not very good quality', 'Very beautiful!', 'Confusing product in sizes', 'Awesome!', 'Stylish and lightweight', 'EXCELLENT', 'Simply beautiful slippers', 'Durable.', 'Slippers for long cold winter times.']\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (270) does not match length of index (2994)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:46\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\AlinaCastell\\OneDrive - Clear Peaks SL\\Desktop\\workspace_general\\.venvtransformers\\Lib\\site-packages\\pandas\\core\\frame.py:4299\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4296\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setitem_array([key], value)\n\u001b[0;32m   4297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   4298\u001b[0m     \u001b[38;5;66;03m# set column\u001b[39;00m\n\u001b[1;32m-> 4299\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_item\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\AlinaCastell\\OneDrive - Clear Peaks SL\\Desktop\\workspace_general\\.venvtransformers\\Lib\\site-packages\\pandas\\core\\frame.py:4512\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m   4502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_set_item\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   4503\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4504\u001b[0m \u001b[38;5;124;03m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[0;32m   4505\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4510\u001b[0m \u001b[38;5;124;03m    ensure homogeneity.\u001b[39;00m\n\u001b[0;32m   4511\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4512\u001b[0m     value, refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sanitize_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4514\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4515\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m   4516\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   4517\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value\u001b[38;5;241m.\u001b[39mdtype, ExtensionDtype)\n\u001b[0;32m   4518\u001b[0m     ):\n\u001b[0;32m   4519\u001b[0m         \u001b[38;5;66;03m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[0;32m   4520\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[1;32mc:\\Users\\AlinaCastell\\OneDrive - Clear Peaks SL\\Desktop\\workspace_general\\.venvtransformers\\Lib\\site-packages\\pandas\\core\\frame.py:5253\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m   5250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reindex_for_setitem(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m   5252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_list_like(value):\n\u001b[1;32m-> 5253\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5254\u001b[0m arr \u001b[38;5;241m=\u001b[39m sanitize_array(value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, allow_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   5255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   5256\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(value, Index)\n\u001b[0;32m   5257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m value\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5260\u001b[0m     \u001b[38;5;66;03m# TODO: Remove kludge in sanitize_array for string mode when enforcing\u001b[39;00m\n\u001b[0;32m   5261\u001b[0m     \u001b[38;5;66;03m# this deprecation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\AlinaCastell\\OneDrive - Clear Peaks SL\\Desktop\\workspace_general\\.venvtransformers\\Lib\\site-packages\\pandas\\core\\common.py:573\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    570\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 573\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    574\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    575\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    577\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    578\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (270) does not match length of index (2994)"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# MAIN PIPELINE\n",
    "# Input: raw csv dataset of shape (6823, 11)\n",
    "# Steps: \n",
    "#       Create dataframe of shape (6823, 4)\n",
    "#       Identify row language by creating new column\n",
    "#       Translate rows using three methods adding new columns\n",
    "#       Evaluate translations using three metrics\n",
    "# Output: Evaluation metrics results for each method and metric\n",
    "\n",
    "\n",
    "# STEP 1: Data Preprocessing\n",
    "# Load dataset \n",
    "filename = './data/raw/amazon_uk_dataset.csv'\n",
    "cols = ['product_name','review_title','review_text','review_rating']\n",
    "\n",
    "complete_data, non_en_data = preprocess_data(filename, cols)\n",
    "print(f\"portion of English data is {len(complete_data)} wrt non English {len(non_en_data)}\\n\")\n",
    "print(f\"preprocessed non English data is {non_en_data.head(10)}\\n\")\n",
    "to_translate_col = non_en_data['review_title']\n",
    "lang_col = non_en_data['language']\n",
    "\n",
    "\n",
    "# STEP 1.2: Batch creation for the next steps\n",
    "num_batches = 27 # 3 times the RPM, as 3 is RPM\n",
    "# Create 'batch_id' column using pd.cut\n",
    "non_en_data['batch_id'] = pd.cut(non_en_data.index, bins=num_batches, labels=range(1, num_batches + 1))\n",
    "\n",
    "# STEP 2: Translation using OpenAI GPT-3.5 model (Gold Standard)\n",
    "goldstd_filename = './data/preprocessed/goldstd_data.csv'\n",
    "\n",
    "# Check if Gold Standard column has been computed\n",
    "if not os.path.exists(goldstd_filename):\n",
    "    # Empty list to fill with OpenAIGPT translations\n",
    "    translated_openaigpt = []\n",
    "\n",
    "    for batch_num in range(1, num_batches + 1):\n",
    "        # Select rows corresponding to the batch number\n",
    "        df_aux = non_en_data[non_en_data['batch_id'] == batch_num]\n",
    "        print(f\"check new df {len(df_aux)}\\n\")\n",
    "        # Apply translation function\n",
    "        translated_batch = translate_openaigpt(df_aux['review_title'], df_aux['language'])\n",
    "        print(f\"translated batch is {translated_batch}\\n\")\n",
    "        translated_openaigpt.extend(translated_batch)\n",
    "\n",
    "    non_en_data['translated_openaigpt'] = translated_openaigpt\n",
    "    non_en_data.to_csv(goldstd_filename, index=False)\n",
    "    print(f\"translated df saved!\\n\")\n",
    "else:\n",
    "    non_en_data = pd.read_csv(goldstd_filename, delimiter=',', index_col=None, header=0)\n",
    "\n",
    "'''   \n",
    "# STEP 3: Translation using Helsinki model\n",
    "translated_helsinki = translate_helsinki(to_translate_col, lang_col)\n",
    "non_en_data['translated_helsinki'] = translated_helsinki\n",
    "\n",
    "# STEP 4: Translation using Facebook models\n",
    "translated_fbnllb = translate_facebook_nllb(to_translate_col, lang_col)\n",
    "non_en_data['translated_facebook_nllb'] = translated_facebook_nllb\n",
    "\n",
    "# STEP 5: Evaluation of models' performance\n",
    "# matrix ? \n",
    "evaluation_results = evaluate_performance(metric1_result, metric2_result, metric3_result)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "270"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(translated_openaigpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venvtransformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
